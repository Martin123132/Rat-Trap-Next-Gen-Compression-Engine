"""mbt2_tool.py - Stand-alone script providing MBT2 archive functionality.

This single Python script includes the MBT2 storage implementation for voxel data
(with recommended defaults for maximal compaction) - this allows writing and reading
voxel records using Z-order keys.

It provides functions for:
- Writing MBT2 archives (`write_mbt2`)
- Opening MBT2 archives for reading (`open_reader`)
- Looking up individual records by key (`lookup`)
- Performing a full scan of records (`full_scan`)

Note: This script focuses specifically on the MBT2 format designed for structured
voxel-like data and does not include the generic folder compression/extraction
functionality found in gmw_tool.py.

To use:
1. Define your data as a list of MBT2Record objects or tuples (key, mass, entropy, tau).
2. Call `write_mbt2` to create an MBT2 archive.
3. Use `open_reader`, `lookup`, or `full_scan` to read data from an existing archive.
"""

import os
import io
import struct
import json
import time
import threading
import zlib
from dataclasses import dataclass
from pathlib import Path
from typing import List, Tuple, Optional, Iterable, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

# Optional imports for Zstandard, AESGCM, and xxHash
try:
    import zstandard as zstd  # optional
    HAS_ZSTD = True
except Exception:
    zstd = None
    HAS_ZSTD = False

try:
    from cryptography.hazmat.primitives.ciphers.aead import AESGCM  # optional
    HAS_AESGCM = True
except Exception:
    AESGCM = None
    HAS_AESGCM = False

try:
    import xxhash
    HAS_XX = True
except Exception:
    xxhash = None
    HAS_XX = False

import hashlib

# -----------------------------------------------------------------------------
# Morton encoding helpers for voxel keys

_DILATE16 = [0] * 65536

def _init_dilate():
    for i in range(65536):
        n = i
        n = (n | (n << 16)) & 0x0000FFFF0000FFFF
        n = (n | (n << 8)) & 0x00FF00FF00FF00FF
        n = (n | (n << 4)) & 0x0F0F0F0F0F0F0F0F
        n = (n | (n << 2)) & 0x3333333333333333
        n = (n | (n << 1)) & 0x5555555555555555
        _DILATE16[i] = n

# Ensure dilate is initialized
if _DILATE16[1] == 0: # Check if it's still the initial state
    _init_dilate()

def morton64(x: int, y: int, z: int) -> int:
    """Encodes a 3D coordinate (x, y, z) into a 64-bit Morton key."""
    x &= 0x1FFFFF
    y &= 0x1FFFFF
    z &= 0x1FFFFF
    xx = _DILATE16[x & 0xFFFF] | (_DILATE16[(x >> 16) & 0xFFFF] << 32)
    yy = _DILATE16[y & 0xFFFF] | (_DILATE16[(y >> 16) & 0xFFFF] << 32)
    zz = _DILATE16[z & 0xFFFF] | (_DILATE16[(z >> 16) & 0xFFFF] << 32)
    return (xx | (yy << 1) | (zz << 2)) & ((1 << 64) - 1)

def inv_morton64(k: int) -> Tuple[int, int, int]:
    """Decodes a 64-bit Morton key back into a 3D coordinate (x, y, z)."""
    def compact(n: int) -> int:
        n &= 0x5555555555555555
        n = (n | (n >> 1)) & 0x3333333333333333
        n = (n | (n >> 2)) & 0x0F0F0F0F0F0F0F0F
        n = (n | (n >> 4)) & 0x00FF00FF00FF00FF
        n = (n | (n >> 8)) & 0x0000FFFF0000FFFF
        n = (n | (n >> 16)) & 0x00000000FFFFFFFF
        return n
    x = compact(k) & 0x1FFFFF
    y = compact(k >> 1) & 0x1FFFFF
    z = compact(k >> 2) & 0x1FFFFF
    return x, y, z

# Varint helpers

def enc_varint(n: int) -> bytes:
    """Encodes an integer into a variable-length byte sequence (varint)."""
    out = bytearray()
    while True:
        b = n & 0x7F
        n >>= 7
        if n:
            out.append(0x80 | b)
        else:
            out.append(b)
            break
    return bytes(out)

def dec_varint(buf: bytes, off: int) -> Tuple[int, int]:
    """Decodes a variable-length byte sequence (varint) from a buffer."""
    shift = 0
    val = 0
    while True:
        b = buf[off]
        off += 1
        val |= (b & 0x7F) << shift
        if not (b & 0x80):
            break
        shift += 7
    return val, off

# Bloom filter hash (deterministic)

def _bloom_hash(key: int, i: int) -> int:
    """Deterministic hash function for bloom filter."""
    x = (key + 0x9E3779B97F4A7C15 + i) & 0xFFFFFFFFFFFFFFFF
    x = (x ^ (x >> 30)) * 0xBF58476D1CE4E5B9 & 0xFFFFFFFFFFFFFFFF
    x = (x ^ (x >> 27)) * 0x94D049BB133111EB & 0xFFFFFFFFFFFFFFFF
    x = x ^ (x >> 31)
    return x & 0xFFFFFFFF

def _bloom_add(bits: bytearray, key: int, m: int, h: int = 3) -> None:
    """Adds a key to the bloom filter."""
    for i in range(h):
        hv = _bloom_hash(key, i) % m
        bits[hv // 8] |= 1 << (hv % 8)

def _bloom_maybe(bits: bytes, key: int, m: int, h: int = 3) -> bool:
    """Checks if a key might be in the bloom filter (with potential false positives)."""
    for i in range(h):
        hv = _bloom_hash(key, i) % m
        if not (bits[hv // 8] & (1 << (hv % 8))):
            return False
    return True

# Format constants

MAGIC = b"MBT2\0"
VERSION = 2

FLAG_ZLIB = 1 << 0
FLAG_ZSTD = 1 << 1
FLAG_AESGCM = 1 << 2
FLAG_VARINT = 1 << 3
FLAG_COLUMN = 1 << 4
FLAG_HILBERT = 1 << 5
FLAG_KEY_128 = 1 << 6

HEADER = struct.Struct("<5sB I H I Q")
ENTRY = struct.Struct("<Q I Q I Q 32s H I")
FOOTER = struct.Struct("<I 32s 32s I")

@dataclass
class WriterCfg:
    """Recommended configuration for writing MBT2 archives."""
    target_bucket_kb: int = 128
    use_zstd: bool = True
    zstd_level: int = 3
    threads: int = max(1, os.cpu_count() or 1)
    aesgcm_key: Optional[bytes] = None
    sha256_each: bool = False
    bloom_bits: int = 2048
    column_layout: bool = True

@dataclass
class MBT2Record:
    """Represents a single MBT2 record with physics components."""
    key: int     # Morton key (space)
    mass: float  # Motion (resistance)
    entropy: float # Entropy (time)
    tau: float   # Curvature memory (time-space)


def _compress_data(raw: bytes, zstd_on: bool, lvl: int = 3) -> bytes:
    """Compresses raw data using Zstandard or zlib."""
    if zstd_on and HAS_ZSTD:
        return zstd.ZstdCompressor(level=lvl).compress(raw)
    return zlib.compress(raw, 6)

def _decompress_data(comp: bytes) -> bytes:
    """Decompresses data using Zstandard or zlib."""
    if HAS_ZSTD and len(comp) >= 4 and comp[:4] == b"\x28\xB5\x2F\xFD":
        return zstd.ZstdDecompressor().decompress(comp)
    return zlib.decompress(comp)

def _maybe_encrypt(aes_key: Optional[bytes], data: bytes, aad: bytes) -> bytes:
    """Encrypts data using AESGCM if a key is provided and AESGCM is available."""
    if not aes_key or not HAS_AESGCM:
        return data
    nonce = os.urandom(12)
    return nonce + AESGCM(aes_key).encrypt(nonce, data, aad)

def _maybe_decrypt(aes_key: Optional[bytes], data: bytes, aad: bytes) -> bytes:
    """Decrypts data using AESGCM if a key is provided and AESGCM is available."""
    if not aes_key or not HAS_AESGCM:
        return data
    nonce, ct = data[:12], data[12:]
    return AESGCM(aes_key).decrypt(nonce, ct, aad)

def _xx64(b: bytes) -> int:
    """Calculates a 64-bit hash of bytes using xxHash or a fallback."""
    if HAS_XX:
        return xxhash.xxh64(b).intdigest()
    return int.from_bytes(hashlib.blake2b(b, digest_size=8).digest(), "little")

def _autotune_bucket_kb(n: int, approx_rec_bytes: int = 20) -> int:
    """Autotunes the target bucket size based on the number of records."""
    recs = max(4096, min(12288, n // 80))
    kb = max(64, min(256, (recs * approx_rec_bytes) // 1024))
    return int(kb)

def _pack_bucket(keys: List[int], masses: List[float], ents: List[float], taus: List[float], column: bool = True) -> bytes:
    """Packs a bucket of record data into bytes."""
    n = len(keys)
    if n == 0:
        return b""
    out = bytearray()
    out += struct.pack("<Q", keys[0])
    prev = keys[0]
    for i in range(1, n):
        d = keys[i] - prev
        out += enc_varint(d)
        prev = keys[i]
    if column:
        out += struct.pack(f"<{n}f", *masses)
        out += struct.pack(f"<{n}f", *ents)
        out += struct.pack(f"<{n}f", *taus)
    else:
        for i in range(n):
            out += struct.pack("<fff", masses[i], ents[i], taus[i])
    return bytes(out)

def _build_buckets(sorted_recs: List[Tuple[int, float, float, float]], target_kb: int) -> List[Tuple[List[int], List[float], List[float], List[float]]]:
    """Builds data buckets from sorted records based on a target size."""
    target = target_kb * 1024
    out = []
    ks: List[int] = []
    ms: List[float] = []
    es: List[float] = []
    ts: List[float] = []
    size = 0
    for k, m, e, t in sorted_recs:
        add = (3 if ks else 8) + 12
        if size + add > target and ks:
            out.append((ks, ms, es, ts))
            ks, ms, es, ts = [], [], [], []
            size = 0
        ks.append(k)
        ms.append(m)
        es.append(e)
        ts.append(t)
        size += add
    if ks:
        out.append((ks, ms, es, ts))
    return out

def write_mbt2(path: Path, records: List[Tuple[int, float, float, float]], cfg: WriterCfg = WriterCfg()) -> Dict:
    """Writes a list of records to an MBT2 archive file."""
    path = Path(path)
    t_sort0 = time.perf_counter()
    records.sort(key=lambda r: r[0])
    t_sort = time.perf_counter() - t_sort0
    if cfg.target_bucket_kb <= 0:
        cfg.target_bucket_kb = _autotune_bucket_kb(len(records))
    buckets = _build_buckets(records, cfg.target_bucket_kb)
    nb = len(buckets)
    flags = 0
    flags |= FLAG_ZSTD if (cfg.use_zstd and HAS_ZSTD) else FLAG_ZLIB
    flags |= FLAG_VARINT
    if cfg.column_layout:
        flags |= FLAG_COLUMN
    if cfg.aesgcm_key and HAS_AESGCM:
        flags |= FLAG_AESGCM
    with open(path, "w+b") as f:
        f.write(HEADER.pack(MAGIC, VERSION, flags, cfg.target_bucket_kb, nb, 0))
        header_end = f.tell()
        bloom_bytes = cfg.bloom_bits // 8
        entry_size = ENTRY.size + bloom_bytes
        table_ofs = header_end
        f.seek(table_ofs + nb * entry_size)
        metas = []
        lock = threading.Lock()
        def write_bucket(i: int, b) -> None:
            ks, ms, es, ts = b
            raw = _pack_bucket(ks, ms, es, ts, column=bool(flags & FLAG_COLUMN))
            comp = _compress_data(raw, zstd_on=bool(flags & FLAG_ZSTD), lvl=cfg.zstd_level)
            comp = _maybe_encrypt(cfg.aesgcm_key if (flags & FLAG_AESGCM) else None, comp, aad=struct.pack("<I", i))
            xx = _xx64(comp)
            sh = hashlib.sha256(comp).digest() if cfg.sha256_each else b"\x00" * 32
            bloom = bytearray(bloom_bytes)
            for kk in ks:
                _bloom_add(bloom, kk, cfg.bloom_bits)
            with lock:
                pos = f.tell()
                f.write(comp)
                metas.append((ks[0], len(ks), pos, len(comp), xx, sh, bytes(bloom)))
        t_comp0 = time.perf_counter()
        with ThreadPoolExecutor(max_workers=cfg.threads) as ex:
            futs = [ex.submit(write_bucket, i, b) for i, b in enumerate(buckets)]
            for fu in as_completed(futs):
                fu.result()
        t_comp = time.perf_counter() - t_comp0
        metas.sort(key=lambda m: m[0])
        data_end = f.tell()
        f.seek(table_ofs)
        for (start, count, off, length, xx, sh, bloom) in metas:
            f.write(ENTRY.pack(start, count, off, length, xx, sh, len(bloom), 0))
            f.write(bloom)
        f.seek(table_ofs)
        table_blob = f.read(nb * entry_size)
        table_hash = hashlib.sha256(table_blob).digest()
        leaves = [sh if cfg.sha256_each else xx.to_bytes(8, "little") for (_, _, _, _, xx, sh, _) in metas]
        if leaves:
            cur = [hashlib.sha256(x).digest() if len(x) != 32 else x for x in leaves]
            while len(cur) > 1:
                nxt = []
                for i in range(0, len(cur), 2):
                    a = cur[i]
                    b = cur[i + 1] if i + 1 < len(cur) else a
                    nxt.append(hashlib.sha256(a + b).digest())
                cur = nxt
            merkle = cur[0]
        else:
            merkle = b"\x00" * 32
        f.seek(data_end)
        feature_crc = zlib.crc32(struct.pack("<IHH", VERSION, cfg.target_bucket_kb, flags)) & 0xFFFFFFFF
        f.write(FOOTER.pack(nb * entry_size, table_hash, merkle, feature_crc))
        end_pos = f.tell()
        f.seek(0)
        f.write(HEADER.pack(MAGIC, VERSION, flags, cfg.target_bucket_kb, nb, table_ofs))
        f.seek(end_pos)
        manifest = {
            "nbuckets": nb,
            "schema": 0,
            "flags": flags,
            "target_bucket_kb": cfg.target_bucket_kb,
            "compressor": "zstd" if (flags & FLAG_ZSTD) else "zlib",
            "aesgcm": bool(flags & FLAG_AESGCM),
            "column": bool(flags & FLAG_COLUMN),
            "varint": bool(flags & FLAG_VARINT),
            "bloom_bits": cfg.bloom_bits,
        }
        man_bytes = json.dumps(manifest, separators=(",", ":")).encode("utf-8")
        f.write(man_bytes)
        f.write(struct.pack("<I", len(man_bytes)))
    return {
        "file_mb": os.path.getsize(path) / (1024 * 1024),
        "nbuckets": nb,
        "bucket_kb": cfg.target_bucket_kb,
        "flags": flags,
    }

@dataclass
class Reader:
    """Reader for MBT2 archive files."""
    path: Path
    flags: int
    bucket_kb: int
    nbuckets: int
    table_pos: int
    entry_size: int
    index: List[Tuple[int, int, int, int]]
    blooms: List[bytes]
    f: io.BufferedReader

def open_reader(path: Path) -> Reader:
    """Opens an MBT2 archive file for reading."""
    f = open(path, "rb")
    hdr = f.read(HEADER.size)
    magic, ver, flags, bucket_kb, nb, table_pos = HEADER.unpack(hdr)
    assert magic == MAGIC and ver == VERSION, "Invalid MBT2 file"
    try:
        f.seek(-4, os.SEEK_END)
        mlen = struct.unpack("<I", f.read(4))[0]
        f.seek(-4 - mlen, os.SEEK_END)
        man_bytes = f.read(mlen)
        manifest = json.loads(man_bytes)
        bloom_bits = manifest.get("bloom_bits", 2048)
    except Exception:
        bloom_bits = 2048
    bloom_bytes = bloom_bits // 8
    entry_size = ENTRY.size + bloom_bytes
    f.seek(table_pos)
    index = []
    blooms = []
    for _ in range(nb):
        start, count, off, length, xx, sh, bb_len, stats_off = ENTRY.unpack(f.read(ENTRY.size))
        bloom = f.read(bb_len)
        index.append((start, count, off, length))
        blooms.append(bloom)
    return Reader(Path(path), flags, bucket_kb, nb, table_pos, entry_size, index, blooms, f)

def _bucket_for(index: List[Tuple[int, int, int, int]], key: int) -> int:
    """Finds the index of the bucket that potentially contains the given key."""
    lo, hi = 0, len(index) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if index[mid][0] <= key and (mid == len(index) - 1 or index[mid + 1][0] > key):
            return mid
        if index[mid][0] < key:
            lo = mid + 1
        else:
            hi = mid - 1
    return max(0, min(len(index) - 1, lo))

def lookup(reader: Reader, x: int, y: int, z: int, aes_key: Optional[bytes] = None) -> Optional[Tuple[float, float, float]]:
    """Looks up a record by its 3D coordinate (x, y, z) in an MBT2 archive."""
    key = morton64(x, y, z)
    i = _bucket_for(reader.index, key)
    start, count, off, length = reader.index[i]
    bloom = reader.blooms[i]
    if not _bloom_maybe(bloom, key, 8 * len(bloom)):
        return None
    reader.f.seek(off)
    comp = reader.f.read(length)
    comp = _maybe_decrypt(aes_key if (reader.flags & FLAG_AESGCM) else None, comp, struct.pack("<I", i))
    raw = _decompress_data(comp)
    p = 0
    first = struct.unpack_from("<Q", raw, p)[0]
    p += 8
    cur = first
    pos = 0
    while pos < count and cur < key:
        d, p = dec_varint(raw, p)
        cur += d
        pos += 1
    if cur != key:
        return None
    for _ in range(pos, count - 1):
        _, p = dec_varint(raw, p)
    fsz = 4 * count
    masses_off = p
    ents_off = p + fsz
    taus_off = p + 2 * fsz
    j = pos
    mass = struct.unpack_from("<f", raw, masses_off + 4 * j)[0]
    ent = struct.unpack_from("<f", raw, ents_off + 4 * j)[0]
    tau = struct.unpack_from("<f", raw, taus_off + 4 * j)[0]
    return mass, ent, tau

def full_scan(reader: Reader) -> Tuple[int, float]:
    """Performs a full scan of all records in an MBT2 archive."""
    total = 0
    mass_sum = 0.0
    for (start, count, off, length), bloom in zip(reader.index, reader.blooms):
        reader.f.seek(off)
        comp = reader.f.read(length)
        raw = _decompress_data(comp)
        p = 8
        for _ in range(count - 1):
            _, p = dec_varint(raw, p)
        masses = struct.unpack_from(f"<{count}f", raw, p)
        total += count
        mass_sum += float(sum(masses))
    return total, mass_sum

# Example Usage (Optional - uncomment to run)
# if __name__ == "__main__":
#     # Example of creating and reading an MBT2 archive
#     print("Creating example MBT2 archive...")
#     example_records = [
#         (morton64(10, 20, 30), 1.5, 0.1, 0.05),
#         (morton64(5, 15, 25), 2.0, 0.2, 0.1),
#         (morton64(12, 22, 32), 1.8, 0.15, 0.08),
#     ]
#     output_mbt2_file = "example_archive.mbt2"
#     write_mbt2(Path(output_mbt2_file), example_records)
#     print(f"Example archive created: {output_mbt2_file}")

#     print("\nReading from example MBT2 archive...")
#     try:
#         mbt2_reader = open_reader(Path(output_mbt2_file))

#         # Lookup a record
#         x, y, z = 10, 20, 30
#         record = lookup(mbt2_reader, x, y, z)
#         if record:
#             print(f"Lookup for ({x}, {y}, {z}): Mass={record[0]}, Entropy={record[1]}, Tau={record[2]}")
#         else:
#             print(f"Record not found for ({x}, {y}, {z})")

#         # Full scan (example)
#         # total_records, total_mass = full_scan(mbt2_reader)
#         # print(f"Full scan: Total records = {total_records}, Total mass = {total_mass}")

#         mbt2_reader.f.close() # Close the file
#     except Exception as e:
#         print(f"Error reading MBT2 archive: {e}")

#     # Clean up example file
#     if os.path.exists(output_mbt2_file):
#         os.remove(output_mbt2_file)
#         print(f"Removed example archive: {output_mbt2_file}")



















                          

"""mbt2_tool.py - Stand-alone script providing MBT2 archive functionality.

This single Python script includes the MBT2 storage implementation for voxel data
(with recommended defaults for maximal compaction) - this allows writing and reading
voxel records using Z-order keys.

It provides functions for:
- Writing MBT2 archives (`write_mbt2`)
- Opening MBT2 archives for reading (`open_reader`)
- Looking up individual records by key (`lookup`)
- Performing a full scan of records (`full_scan`)

Note: This script focuses specifically on the MBT2 format designed for structured
voxel-like data and does not include the generic folder compression/extraction
functionality found in gmw_tool.py.

To use:
1. Define your data as a list of MBT2Record objects or tuples (key, mass, entropy, tau).
2. Call `write_mbt2` to create an MBT2 archive.
3. Use `open_reader`, `lookup`, or `full_scan` to read data from an existing archive.
"""

import os
import io
import struct
import json
import time
import threading
import zlib
from dataclasses import dataclass
from pathlib import Path
from typing import List, Tuple, Optional, Iterable, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

# Optional imports for Zstandard, AESGCM, and xxHash
try:
    import zstandard as zstd  # optional
    HAS_ZSTD = True
except Exception:
    zstd = None
    HAS_ZSTD = False

try:
    from cryptography.hazmat.primitives.ciphers.aead import AESGCM  # optional
    HAS_AESGCM = True
except Exception:
    AESGCM = None
    HAS_AESGCM = False

try:
    import xxhash
    HAS_XX = True
except Exception:
    xxhash = None
    HAS_XX = False

import hashlib

# -----------------------------------------------------------------------------
# Morton encoding helpers for voxel keys

_DILATE16 = [0] * 65536

def _init_dilate():
    for i in range(65536):
        n = i
        n = (n | (n << 16)) & 0x0000FFFF0000FFFF
        n = (n | (n << 8)) & 0x00FF00FF00FF00FF
        n = (n | (n << 4)) & 0x0F0F0F0F0F0F0F0F
        n = (n | (n << 2)) & 0x3333333333333333
        n = (n | (n << 1)) & 0x5555555555555555
        _DILATE16[i] = n

# Ensure dilate is initialized
if _DILATE16[1] == 0: # Check if it's still the initial state
    _init_dilate()

def morton64(x: int, y: int, z: int) -> int:
    """Encodes a 3D coordinate (x, y, z) into a 64-bit Morton key."""
    x &= 0x1FFFFF
    y &= 0x1FFFFF
    z &= 0x1FFFFF
    xx = _DILATE16[x & 0xFFFF] | (_DILATE16[(x >> 16) & 0xFFFF] << 32)
    yy = _DILATE16[y & 0xFFFF] | (_DILATE16[(y >> 16) & 0xFFFF] << 32)
    zz = _DILATE16[z & 0xFFFF] | (_DILATE16[(z >> 16) & 0xFFFF] << 32)
    return (xx | (yy << 1) | (zz << 2)) & ((1 << 64) - 1)

def inv_morton64(k: int) -> Tuple[int, int, int]:
    """Decodes a 64-bit Morton key back into a 3D coordinate (x, y, z)."""
    def compact(n: int) -> int:
        n &= 0x5555555555555555
        n = (n | (n >> 1)) & 0x3333333333333333
        n = (n | (n >> 2)) & 0x0F0F0F0F0F0F0F0F
        n = (n | (n >> 4)) & 0x00FF00FF00FF00FF
        n = (n | (n >> 8)) & 0x0000FFFF0000FFFF
        n = (n | (n >> 16)) & 0x00000000FFFFFFFF
        return n
    x = compact(k) & 0x1FFFFF
    y = compact(k >> 1) & 0x1FFFFF
    z = compact(k >> 2) & 0x1FFFFF
    return x, y, z

# Varint helpers

def enc_varint(n: int) -> bytes:
    """Encodes an integer into a variable-length byte sequence (varint)."""
    out = bytearray()
    while True:
        b = n & 0x7F
        n >>= 7
        if n:
            out.append(0x80 | b)
        else:
            out.append(b)
            break
    return bytes(out)

def dec_varint(buf: bytes, off: int) -> Tuple[int, int]:
    """Decodes a variable-length byte sequence (varint) from a buffer."""
    shift = 0
    val = 0
    while True:
        b = buf[off]
        off += 1
        val |= (b & 0x7F) << shift
        if not (b & 0x80):
            break
        shift += 7
    return val, off

# Bloom filter hash (deterministic)

def _bloom_hash(key: int, i: int) -> int:
    """Deterministic hash function for bloom filter."""
    x = (key + 0x9E3779B97F4A7C15 + i) & 0xFFFFFFFFFFFFFFFF
    x = (x ^ (x >> 30)) * 0xBF58476D1CE4E5B9 & 0xFFFFFFFFFFFFFFFF
    x = (x ^ (x >> 27)) * 0x94D049BB133111EB & 0xFFFFFFFFFFFFFFFF
    x = x ^ (x >> 31)
    return x & 0xFFFFFFFF

def _bloom_add(bits: bytearray, key: int, m: int, h: int = 3) -> None:
    """Adds a key to the bloom filter."""
    for i in range(h):
        hv = _bloom_hash(key, i) % m
        bits[hv // 8] |= 1 << (hv % 8)

def _bloom_maybe(bits: bytes, key: int, m: int, h: int = 3) -> bool:
    """Checks if a key might be in the bloom filter (with potential false positives)."""
    for i in range(h):
        hv = _bloom_hash(key, i) % m
        if not (bits[hv // 8] & (1 << (hv % 8))):
            return False
    return True

# Format constants

MAGIC = b"MBT2\0"
VERSION = 2

FLAG_ZLIB = 1 << 0
FLAG_ZSTD = 1 << 1
FLAG_AESGCM = 1 << 2
FLAG_VARINT = 1 << 3
FLAG_COLUMN = 1 << 4
FLAG_HILBERT = 1 << 5
FLAG_KEY_128 = 1 << 6

HEADER = struct.Struct("<5sB I H I Q")
ENTRY = struct.Struct("<Q I Q I Q 32s H I")
FOOTER = struct.Struct("<I 32s 32s I")

@dataclass
class WriterCfg:
    """Recommended configuration for writing MBT2 archives."""
    target_bucket_kb: int = 128
    use_zstd: bool = True
    zstd_level: int = 3
    threads: int = max(1, os.cpu_count() or 1)
    aesgcm_key: Optional[bytes] = None
    sha256_each: bool = False
    bloom_bits: int = 2048
    column_layout: bool = True

@dataclass
class MBT2Record:
    """Represents a single MBT2 record with physics components."""
    key: int     # Morton key (space)
    mass: float  # Motion (resistance)
    entropy: float # Entropy (time)
    tau: float   # Curvature memory (time-space)


def _compress_data(raw: bytes, zstd_on: bool, lvl: int = 3) -> bytes:
    """Compresses raw data using Zstandard or zlib."""
    if zstd_on and HAS_ZSTD:
        return zstd.ZstdCompressor(level=lvl).compress(raw)
    return zlib.compress(raw, 6)

def _decompress_data(comp: bytes) -> bytes:
    """Decompresses data using Zstandard or zlib."""
    if HAS_ZSTD and len(comp) >= 4 and comp[:4] == b"\x28\xB5\x2F\xFD":
        return zstd.ZstdDecompressor().decompress(comp)
    return zlib.decompress(comp)

def _maybe_encrypt(aes_key: Optional[bytes], data: bytes, aad: bytes) -> bytes:
    """Encrypts data using AESGCM if a key is provided and AESGCM is available."""
    if not aes_key or not HAS_AESGCM:
        return data
    nonce = os.urandom(12)
    return nonce + AESGCM(aes_key).encrypt(nonce, data, aad)

def _maybe_decrypt(aes_key: Optional[bytes], data: bytes, aad: bytes) -> bytes:
    """Decrypts data using AESGCM if a key is provided and AESGCM is available."""
    if not aes_key or not HAS_AESGCM:
        return data
    nonce, ct = data[:12], data[12:]
    return AESGCM(aes_key).decrypt(nonce, ct, aad)

def _xx64(b: bytes) -> int:
    """Calculates a 64-bit hash of bytes using xxHash or a fallback."""
    if HAS_XX:
        return xxhash.xxh64(b).intdigest()
    return int.from_bytes(hashlib.blake2b(b, digest_size=8).digest(), "little")

def _autotune_bucket_kb(n: int, approx_rec_bytes: int = 20) -> int:
    """Autotunes the target bucket size based on the number of records."""
    recs = max(4096, min(12288, n // 80))
    kb = max(64, min(256, (recs * approx_rec_bytes) // 1024))
    return int(kb)

def _pack_bucket(keys: List[int], masses: List[float], ents: List[float], taus: List[float], column: bool = True) -> bytes:
    """Packs a bucket of record data into bytes."""
    n = len(keys)
    if n == 0:
        return b""
    out = bytearray()
    out += struct.pack("<Q", keys[0])
    prev = keys[0]
    for i in range(1, n):
        d = keys[i] - prev
        out += enc_varint(d)
        prev = keys[i]
    if column:
        out += struct.pack(f"<{n}f", *masses)
        out += struct.pack(f"<{n}f", *ents)
        out += struct.pack(f"<{n}f", *taus)
    else:
        for i in range(n):
            out += struct.pack("<fff", masses[i], ents[i], taus[i])
    return bytes(out)

def _build_buckets(sorted_recs: List[Tuple[int, float, float, float]], target_kb: int) -> List[Tuple[List[int], List[float], List[float], List[float]]]:
    """Builds data buckets from sorted records based on a target size."""
    target = target_kb * 1024
    out = []
    ks: List[int] = []
    ms: List[float] = []
    es: List[float] = []
    ts: List[float] = []
    size = 0
    for k, m, e, t in sorted_recs:
        add = (3 if ks else 8) + 12
        if size + add > target and ks:
            out.append((ks, ms, es, ts))
            ks, ms, es, ts = [], [], [], []
            size = 0
        ks.append(k)
        ms.append(m)
        es.append(e)
        ts.append(t)
        size += add
    if ks:
        out.append((ks, ms, es, ts))
    return out

def write_mbt2(path: Path, records: List[Tuple[int, float, float, float]], cfg: WriterCfg = WriterCfg()) -> Dict:
    """Writes a list of records to an MBT2 archive file."""
    path = Path(path)
    t_sort0 = time.perf_counter()
    records.sort(key=lambda r: r[0])
    t_sort = time.perf_counter() - t_sort0
    if cfg.target_bucket_kb <= 0:
        cfg.target_bucket_kb = _autotune_bucket_kb(len(records))
    buckets = _build_buckets(records, cfg.target_bucket_kb)
    nb = len(buckets)
    flags = 0
    flags |= FLAG_ZSTD if (cfg.use_zstd and HAS_ZSTD) else FLAG_ZLIB
    flags |= FLAG_VARINT
    if cfg.column_layout:
        flags |= FLAG_COLUMN
    if cfg.aesgcm_key and HAS_AESGCM:
        flags |= FLAG_AESGCM
    with open(path, "w+b") as f:
        f.write(HEADER.pack(MAGIC, VERSION, flags, cfg.target_bucket_kb, nb, 0))
        header_end = f.tell()
        bloom_bytes = cfg.bloom_bits // 8
        entry_size = ENTRY.size + bloom_bytes
        table_ofs = header_end
        f.seek(table_ofs + nb * entry_size)
        metas = []
        lock = threading.Lock()
        def write_bucket(i: int, b) -> None:
            ks, ms, es, ts = b
            raw = _pack_bucket(ks, ms, es, ts, column=bool(flags & FLAG_COLUMN))
            comp = _compress_data(raw, zstd_on=bool(flags & FLAG_ZSTD), lvl=cfg.zstd_level)
            comp = _maybe_encrypt(cfg.aesgcm_key if (flags & FLAG_AESGCM) else None, comp, aad=struct.pack("<I", i))
            xx = _xx64(comp)
            sh = hashlib.sha256(comp).digest() if cfg.sha256_each else b"\x00" * 32
            bloom = bytearray(bloom_bytes)
            for kk in ks:
                _bloom_add(bloom, kk, cfg.bloom_bits)
            with lock:
                pos = f.tell()
                f.write(comp)
                metas.append((ks[0], len(ks), pos, len(comp), xx, sh, bytes(bloom)))
        t_comp0 = time.perf_counter()
        with ThreadPoolExecutor(max_workers=cfg.threads) as ex:
            futs = [ex.submit(write_bucket, i, b) for i, b in enumerate(buckets)]
            for fu in as_completed(futs):
                fu.result()
        t_comp = time.perf_counter() - t_comp0
        metas.sort(key=lambda m: m[0])
        data_end = f.tell()
        f.seek(table_ofs)
        for (start, count, off, length, xx, sh, bloom) in metas:
            f.write(ENTRY.pack(start, count, off, length, xx, sh, len(bloom), 0))
            f.write(bloom)
        f.seek(table_ofs)
        table_blob = f.read(nb * entry_size)
        table_hash = hashlib.sha256(table_blob).digest()
        leaves = [sh if cfg.sha256_each else xx.to_bytes(8, "little") for (_, _, _, _, xx, sh, _) in metas]
        if leaves:
            cur = [hashlib.sha256(x).digest() if len(x) != 32 else x for x in leaves]
            while len(cur) > 1:
                nxt = []
                for i in range(0, len(cur), 2):
                    a = cur[i]
                    b = cur[i + 1] if i + 1 < len(cur) else a
                    nxt.append(hashlib.sha256(a + b).digest())
                cur = nxt
            merkle = cur[0]
        else:
            merkle = b"\x00" * 32
        f.seek(data_end)
        feature_crc = zlib.crc32(struct.pack("<IHH", VERSION, cfg.target_bucket_kb, flags)) & 0xFFFFFFFF
        f.write(FOOTER.pack(nb * entry_size, table_hash, merkle, feature_crc))
        end_pos = f.tell()
        f.seek(0)
        f.write(HEADER.pack(MAGIC, VERSION, flags, cfg.target_bucket_kb, nb, table_ofs))
        f.seek(end_pos)
        manifest = {
            "nbuckets": nb,
            "schema": 0,
            "flags": flags,
            "target_bucket_kb": cfg.target_bucket_kb,
            "compressor": "zstd" if (flags & FLAG_ZSTD) else "zlib",
            "aesgcm": bool(flags & FLAG_AESGCM),
            "column": bool(flags & FLAG_COLUMN),
            "varint": bool(flags & FLAG_VARINT),
            "bloom_bits": cfg.bloom_bits,
        }
        man_bytes = json.dumps(manifest, separators=(",", ":")).encode("utf-8")
        f.write(man_bytes)
        f.write(struct.pack("<I", len(man_bytes)))
    return {
        "file_mb": os.path.getsize(path) / (1024 * 1024),
        "nbuckets": nb,
        "bucket_kb": cfg.target_bucket_kb,
        "flags": flags,
    }

@dataclass
class Reader:
    """Reader for MBT2 archive files."""
    path: Path
    flags: int
    bucket_kb: int
    nbuckets: int
    table_pos: int
    entry_size: int
    index: List[Tuple[int, int, int, int]]
    blooms: List[bytes]
    f: io.BufferedReader

def open_reader(path: Path) -> Reader:
    """Opens an MBT2 archive file for reading."""
    f = open(path, "rb")
    hdr = f.read(HEADER.size)
    magic, ver, flags, bucket_kb, nb, table_pos = HEADER.unpack(hdr)
    assert magic == MAGIC and ver == VERSION, "Invalid MBT2 file"
    try:
        f.seek(-4, os.SEEK_END)
        mlen = struct.unpack("<I", f.read(4))[0]
        f.seek(-4 - mlen, os.SEEK_END)
        man_bytes = f.read(mlen)
        manifest = json.loads(man_bytes)
        bloom_bits = manifest.get("bloom_bits", 2048)
    except Exception:
        bloom_bits = 2048
    bloom_bytes = bloom_bits // 8
    entry_size = ENTRY.size + bloom_bytes
    f.seek(table_pos)
    index = []
    blooms = []
    for _ in range(nb):
        start, count, off, length, xx, sh, bb_len, stats_off = ENTRY.unpack(f.read(ENTRY.size))
        bloom = f.read(bb_len)
        index.append((start, count, off, length))
        blooms.append(bloom)
    return Reader(Path(path), flags, bucket_kb, nb, table_pos, entry_size, index, blooms, f)

def _bucket_for(index: List[Tuple[int, int, int, int]], key: int) -> int:
    """Finds the index of the bucket that potentially contains the given key."""
    lo, hi = 0, len(index) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if index[mid][0] <= key and (mid == len(index) - 1 or index[mid + 1][0] > key):
            return mid
        if index[mid][0] < key:
            lo = mid + 1
        else:
            hi = mid - 1
    return max(0, min(len(index) - 1, lo))

def lookup(reader: Reader, x: int, y: int, z: int, aes_key: Optional[bytes] = None) -> Optional[Tuple[float, float, float]]:
    """Looks up a record by its 3D coordinate (x, y, z) in an MBT2 archive."""
    key = morton64(x, y, z)
    i = _bucket_for(reader.index, key)
    start, count, off, length = reader.index[i]
    bloom = reader.blooms[i]
    if not _bloom_maybe(bloom, key, 8 * len(bloom)):
        return None
    reader.f.seek(off)
    comp = reader.f.read(length)
    comp = _maybe_decrypt(aes_key if (reader.flags & FLAG_AESGCM) else None, comp, struct.pack("<I", i))
    raw = _decompress_data(comp)
    p = 0
    first = struct.unpack_from("<Q", raw, p)[0]
    p += 8
    cur = first
    pos = 0
    while pos < count and cur < key:
        d, p = dec_varint(raw, p)
        cur += d
        pos += 1
    if cur != key:
        return None
    for _ in range(pos, count - 1):
        _, p = dec_varint(raw, p)
    fsz = 4 * count
    masses_off = p
    ents_off = p + fsz
    taus_off = p + 2 * fsz
    j = pos
    mass = struct.unpack_from("<f", raw, masses_off + 4 * j)[0]
    ent = struct.unpack_from("<f", raw, ents_off + 4 * j)[0]
    tau = struct.unpack_from("<f", raw, taus_off + 4 * j)[0]
    return mass, ent, tau

def full_scan(reader: Reader) -> Tuple[int, float]:
    """Performs a full scan of all records in an MBT2 archive."""
    total = 0
    mass_sum = 0.0
    for (start, count, off, length), bloom in zip(reader.index, reader.blooms):
        reader.f.seek(off)
        comp = reader.f.read(length)
        raw = _decompress_data(comp)
        p = 8
        for _ in range(count - 1):
            _, p = dec_varint(raw, p)
        masses = struct.unpack_from(f"<{count}f", raw, p)
        total += count
        mass_sum += float(sum(masses))
    return total, mass_sum

# Example Usage (Optional - uncomment to run)
# if __name__ == "__main__":
#     # Example of creating and reading an MBT2 archive
#     print("Creating example MBT2 archive...")
#     example_records = [
#         (morton64(10, 20, 30), 1.5, 0.1, 0.05),
#         (morton64(5, 15, 25), 2.0, 0.2, 0.1),
#         (morton64(12, 22, 32), 1.8, 0.15, 0.08),
#     ]
#     output_mbt2_file = "example_archive.mbt2"
#     write_mbt2(Path(output_mbt2_file), example_records)
#     print(f"Example archive created: {output_mbt2_file}")

#     print("\nReading from example MBT2 archive...")
#     try:
#         mbt2_reader = open_reader(Path(output_mbt2_file))

#         # Lookup a record
#         x, y, z = 10, 20, 30
#         record = lookup(mbt2_reader, x, y, z)
#         if record:
#             print(f"Lookup for ({x}, {y}, {z}): Mass={record[0]}, Entropy={record[1]}, Tau={record[2]}")
#         else:
#             print(f"Record not found for ({x}, {y}, {z})")

#         # Full scan (example)
#         # total_records, total_mass = full_scan(mbt2_reader)
#         # print(f"Full scan: Total records = {total_records}, Total mass = {total_mass}")

#         mbt2_reader.f.close() # Close the file
#     except Exception as e:
#         print(f"Error reading MBT2 archive: {e}")

#     # Clean up example file
#     if os.path.exists(output_mbt2_file):
#         os.remove(output_mbt2_file)
#         print(f"Removed example archive: {output_mbt2_file}")




   """mbt2_tool.py - Stand-alone script providing MBT2 archive functionality.

This single Python script includes the MBT2 storage implementation for voxel data
(with recommended defaults for maximal compaction) - this allows writing and reading
voxel records using Z-order keys.

It provides functions for:
- Writing MBT2 archives (`write_mbt2`)
- Opening MBT2 archives for reading (`open_reader`)
- Looking up individual records by key (`lookup`)
- Performing a full scan of records (`full_scan`)

Note: This script focuses specifically on the MBT2 format designed for structured
voxel-like data and does not include the generic folder compression/extraction
functionality found in gmw_tool.py.

To use:
1. Define your data as a list of MBT2Record objects or tuples (key, mass, entropy, tau).
2. Call `write_mbt2` to create an MBT2 archive.
3. Use `open_reader`, `lookup`, or `full_scan` to read data from an existing archive.
"""

import os
import io
import struct
import json
import time
import threading
import zlib
from dataclasses import dataclass
from pathlib import Path
from typing import List, Tuple, Optional, Iterable, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

# Optional imports for Zstandard, AESGCM, and xxHash
try:
    import zstandard as zstd  # optional
    HAS_ZSTD = True
except Exception:
    zstd = None
    HAS_ZSTD = False

try:
    from cryptography.hazmat.primitives.ciphers.aead import AESGCM  # optional
    HAS_AESGCM = True
except Exception:
    AESGCM = None
    HAS_AESGCM = False

try:
    import xxhash
    HAS_XX = True
except Exception:
    xxhash = None
    HAS_XX = False

import hashlib

# -----------------------------------------------------------------------------
# Morton encoding helpers for voxel keys

_DILATE16 = [0] * 65536

def _init_dilate():
    for i in range(65536):
        n = i
        n = (n | (n << 16)) & 0x0000FFFF0000FFFF
        n = (n | (n << 8)) & 0x00FF00FF00FF00FF
        n = (n | (n << 4)) & 0x0F0F0F0F0F0F0F0F
        n = (n | (n << 2)) & 0x3333333333333333
        n = (n | (n << 1)) & 0x5555555555555555
        _DILATE16[i] = n

# Ensure dilate is initialized
if _DILATE16[1] == 0: # Check if it's still the initial state
    _init_dilate()

def morton64(x: int, y: int, z: int) -> int:
    """Encodes a 3D coordinate (x, y, z) into a 64-bit Morton key."""
    x &= 0x1FFFFF
    y &= 0x1FFFFF
    z &= 0x1FFFFF
    xx = _DILATE16[x & 0xFFFF] | (_DILATE16[(x >> 16) & 0xFFFF] << 32)
    yy = _DILATE16[y & 0xFFFF] | (_DILATE16[(y >> 16) & 0xFFFF] << 32)
    zz = _DILATE16[z & 0xFFFF] | (_DILATE16[(z >> 16) & 0xFFFF] << 32)
    return (xx | (yy << 1) | (zz << 2)) & ((1 << 64) - 1)

def inv_morton64(k: int) -> Tuple[int, int, int]:
    """Decodes a 64-bit Morton key back into a 3D coordinate (x, y, z)."""
    def compact(n: int) -> int:
        n &= 0x5555555555555555
        n = (n | (n >> 1)) & 0x3333333333333333
        n = (n | (n >> 2)) & 0x0F0F0F0F0F0F0F0F
        n = (n | (n >> 4)) & 0x00FF00FF00FF00FF
        n = (n | (n >> 8)) & 0x0000FFFF0000FFFF
        n = (n | (n >> 16)) & 0x00000000FFFFFFFF
        return n
    x = compact(k) & 0x1FFFFF
    y = compact(k >> 1) & 0x1FFFFF
    z = compact(k >> 2) & 0x1FFFFF
    return x, y, z

# Varint helpers

def enc_varint(n: int) -> bytes:
    """Encodes an integer into a variable-length byte sequence (varint)."""
    out = bytearray()
    while True:
        b = n & 0x7F
        n >>= 7
        if n:
            out.append(0x80 | b)
        else:
            out.append(b)
            break
    return bytes(out)

def dec_varint(buf: bytes, off: int) -> Tuple[int, int]:
    """Decodes a variable-length byte sequence (varint) from a buffer."""
    shift = 0
    val = 0
    while True:
        b = buf[off]
        off += 1
        val |= (b & 0x7F) << shift
        if not (b & 0x80):
            break
        shift += 7
    return val, off

# Bloom filter hash (deterministic)

def _bloom_hash(key: int, i: int) -> int:
    """Deterministic hash function for bloom filter."""
    x = (key + 0x9E3779B97F4A7C15 + i) & 0xFFFFFFFFFFFFFFFF
    x = (x ^ (x >> 30)) * 0xBF58476D1CE4E5B9 & 0xFFFFFFFFFFFFFFFF
    x = (x ^ (x >> 27)) * 0x94D049BB133111EB & 0xFFFFFFFFFFFFFFFF
    x = x ^ (x >> 31)
    return x & 0xFFFFFFFF

def _bloom_add(bits: bytearray, key: int, m: int, h: int = 3) -> None:
    """Adds a key to the bloom filter."""
    for i in range(h):
        hv = _bloom_hash(key, i) % m
        bits[hv // 8] |= 1 << (hv % 8)

def _bloom_maybe(bits: bytes, key: int, m: int, h: int = 3) -> bool:
    """Checks if a key might be in the bloom filter (with potential false positives)."""
    for i in range(h):
        hv = _bloom_hash(key, i) % m
        if not (bits[hv // 8] & (1 << (hv % 8))):
            return False
    return True

# Format constants

MAGIC = b"MBT2\0"
VERSION = 2

FLAG_ZLIB = 1 << 0
FLAG_ZSTD = 1 << 1
FLAG_AESGCM = 1 << 2
FLAG_VARINT = 1 << 3
FLAG_COLUMN = 1 << 4
FLAG_HILBERT = 1 << 5
FLAG_KEY_128 = 1 << 6

HEADER = struct.Struct("<5sB I H I Q")
ENTRY = struct.Struct("<Q I Q I Q 32s H I")
FOOTER = struct.Struct("<I 32s 32s I")

@dataclass
class WriterCfg:
    """Recommended configuration for writing MBT2 archives."""
    target_bucket_kb: int = 128
    use_zstd: bool = True
    zstd_level: int = 3
    threads: int = max(1, os.cpu_count() or 1)
    aesgcm_key: Optional[bytes] = None
    sha256_each: bool = False
    bloom_bits: int = 2048
    column_layout: bool = True

@dataclass
class MBT2Record:
    """Represents a single MBT2 record with physics components."""
    key: int     # Morton key (space)
    mass: float  # Motion (resistance)
    entropy: float # Entropy (time)
    tau: float   # Curvature memory (time-space)


def _compress_data(raw: bytes, zstd_on: bool, lvl: int = 3) -> bytes:
    """Compresses raw data using Zstandard or zlib."""
    if zstd_on and HAS_ZSTD:
        return zstd.ZstdCompressor(level=lvl).compress(raw)
    return zlib.compress(raw, 6)

def _decompress_data(comp: bytes) -> bytes:
    """Decompresses data using Zstandard or zlib."""
    if HAS_ZSTD and len(comp) >= 4 and comp[:4] == b"\x28\xB5\x2F\xFD":
        return zstd.ZstdDecompressor().decompress(comp)
    return zlib.decompress(comp)

def _maybe_encrypt(aes_key: Optional[bytes], data: bytes, aad: bytes) -> bytes:
    """Encrypts data using AESGCM if a key is provided and AESGCM is available."""
    if not aes_key or not HAS_AESGCM:
        return data
    nonce = os.urandom(12)
    return nonce + AESGCM(aes_key).encrypt(nonce, data, aad)

def _maybe_decrypt(aes_key: Optional[bytes], data: bytes, aad: bytes) -> bytes:
    """Decrypts data using AESGCM if a key is provided and AESGCM is available."""
    if not aes_key or not HAS_AESGCM:
        return data
    nonce, ct = data[:12], data[12:]
    return AESGCM(aes_key).decrypt(nonce, ct, aad)

def _xx64(b: bytes) -> int:
    """Calculates a 64-bit hash of bytes using xxHash or a fallback."""
    if HAS_XX:
        return xxhash.xxh64(b).intdigest()
    return int.from_bytes(hashlib.blake2b(b, digest_size=8).digest(), "little")

def _autotune_bucket_kb(n: int, approx_rec_bytes: int = 20) -> int:
    """Autotunes the target bucket size based on the number of records."""
    recs = max(4096, min(12288, n // 80))
    kb = max(64, min(256, (recs * approx_rec_bytes) // 1024))
    return int(kb)

def _pack_bucket(keys: List[int], masses: List[float], ents: List[float], taus: List[float], column: bool = True) -> bytes:
    """Packs a bucket of record data into bytes."""
    n = len(keys)
    if n == 0:
        return b""
    out = bytearray()
    out += struct.pack("<Q", keys[0])
    prev = keys[0]
    for i in range(1, n):
        d = keys[i] - prev
        out += enc_varint(d)
        prev = keys[i]
    if column:
        out += struct.pack(f"<{n}f", *masses)
        out += struct.pack(f"<{n}f", *ents)
        out += struct.pack(f"<{n}f", *taus)
    else:
        for i in range(n):
            out += struct.pack("<fff", masses[i], ents[i], taus[i])
    return bytes(out)

def _build_buckets(sorted_recs: List[Tuple[int, float, float, float]], target_kb: int) -> List[Tuple[List[int], List[float], List[float], List[float]]]:
    """Builds data buckets from sorted records based on a target size."""
    target = target_kb * 1024
    out = []
    ks: List[int] = []
    ms: List[float] = []
    es: List[float] = []
    ts: List[float] = []
    size = 0
    for k, m, e, t in sorted_recs:
        add = (3 if ks else 8) + 12
        if size + add > target and ks:
            out.append((ks, ms, es, ts))
            ks, ms, es, ts = [], [], [], []
            size = 0
        ks.append(k)
        ms.append(m)
        es.append(e)
        ts.append(t)
        size += add
    if ks:
        out.append((ks, ms, es, ts))
    return out

def write_mbt2(path: Path, records: List[Tuple[int, float, float, float]], cfg: WriterCfg = WriterCfg()) -> Dict:
    """Writes a list of records to an MBT2 archive file."""
    path = Path(path)
    t_sort0 = time.perf_counter()
    records.sort(key=lambda r: r[0])
    t_sort = time.perf_counter() - t_sort0
    if cfg.target_bucket_kb <= 0:
        cfg.target_bucket_kb = _autotune_bucket_kb(len(records))
    buckets = _build_buckets(records, cfg.target_bucket_kb)
    nb = len(buckets)
    flags = 0
    flags |= FLAG_ZSTD if (cfg.use_zstd and HAS_ZSTD) else FLAG_ZLIB
    flags |= FLAG_VARINT
    if cfg.column_layout:
        flags |= FLAG_COLUMN
    if cfg.aesgcm_key and HAS_AESGCM:
        flags |= FLAG_AESGCM
    with open(path, "w+b") as f:
        f.write(HEADER.pack(MAGIC, VERSION, flags, cfg.target_bucket_kb, nb, 0))
        header_end = f.tell()
        bloom_bytes = cfg.bloom_bits // 8
        entry_size = ENTRY.size + bloom_bytes
        table_ofs = header_end
        f.seek(table_ofs + nb * entry_size)
        metas = []
        lock = threading.Lock()
        def write_bucket(i: int, b) -> None:
            ks, ms, es, ts = b
            raw = _pack_bucket(ks, ms, es, ts, column=bool(flags & FLAG_COLUMN))
            comp = _compress_data(raw, zstd_on=bool(flags & FLAG_ZSTD), lvl=cfg.zstd_level)
            comp = _maybe_encrypt(cfg.aesgcm_key if (flags & FLAG_AESGCM) else None, comp, aad=struct.pack("<I", i))
            xx = _xx64(comp)
            sh = hashlib.sha256(comp).digest() if cfg.sha256_each else b"\x00" * 32
            bloom = bytearray(bloom_bytes)
            for kk in ks:
                _bloom_add(bloom, kk, cfg.bloom_bits)
            with lock:
                pos = f.tell()
                f.write(comp)
                metas.append((ks[0], len(ks), pos, len(comp), xx, sh, bytes(bloom)))
        t_comp0 = time.perf_counter()
        with ThreadPoolExecutor(max_workers=cfg.threads) as ex:
            futs = [ex.submit(write_bucket, i, b) for i, b in enumerate(buckets)]
            for fu in as_completed(futs):
                fu.result()
        t_comp = time.perf_counter() - t_comp0
        metas.sort(key=lambda m: m[0])
        data_end = f.tell()
        f.seek(table_ofs)
        for (start, count, off, length, xx, sh, bloom) in metas:
            f.write(ENTRY.pack(start, count, off, length, xx, sh, len(bloom), 0))
            f.write(bloom)
        f.seek(table_ofs)
        table_blob = f.read(nb * entry_size)
        table_hash = hashlib.sha256(table_blob).digest()
        leaves = [sh if cfg.sha256_each else xx.to_bytes(8, "little") for (_, _, _, _, xx, sh, _) in metas]
        if leaves:
            cur = [hashlib.sha256(x).digest() if len(x) != 32 else x for x in leaves]
            while len(cur) > 1:
                nxt = []
                for i in range(0, len(cur), 2):
                    a = cur[i]
                    b = cur[i + 1] if i + 1 < len(cur) else a
                    nxt.append(hashlib.sha256(a + b).digest())
                cur = nxt
            merkle = cur[0]
        else:
            merkle = b"\x00" * 32
        f.seek(data_end)
        feature_crc = zlib.crc32(struct.pack("<IHH", VERSION, cfg.target_bucket_kb, flags)) & 0xFFFFFFFF
        f.write(FOOTER.pack(nb * entry_size, table_hash, merkle, feature_crc))
        end_pos = f.tell()
        f.seek(0)
        f.write(HEADER.pack(MAGIC, VERSION, flags, cfg.target_bucket_kb, nb, table_ofs))
        f.seek(end_pos)
        manifest = {
            "nbuckets": nb,
            "schema": 0,
            "flags": flags,
            "target_bucket_kb": cfg.target_bucket_kb,
            "compressor": "zstd" if (flags & FLAG_ZSTD) else "zlib",
            "aesgcm": bool(flags & FLAG_AESGCM),
            "column": bool(flags & FLAG_COLUMN),
            "varint": bool(flags & FLAG_VARINT),
            "bloom_bits": cfg.bloom_bits,
        }
        man_bytes = json.dumps(manifest, separators=(",", ":")).encode("utf-8")
        f.write(man_bytes)
        f.write(struct.pack("<I", len(man_bytes)))
    return {
        "file_mb": os.path.getsize(path) / (1024 * 1024),
        "nbuckets": nb,
        "bucket_kb": cfg.target_bucket_kb,
        "flags": flags,
    }

@dataclass
class Reader:
    """Reader for MBT2 archive files."""
    path: Path
    flags: int
    bucket_kb: int
    nbuckets: int
    table_pos: int
    entry_size: int
    index: List[Tuple[int, int, int, int]]
    blooms: List[bytes]
    f: io.BufferedReader

def open_reader(path: Path) -> Reader:
    """Opens an MBT2 archive file for reading."""
    f = open(path, "rb")
    hdr = f.read(HEADER.size)
    magic, ver, flags, bucket_kb, nb, table_pos = HEADER.unpack(hdr)
    assert magic == MAGIC and ver == VERSION, "Invalid MBT2 file"
    try:
        f.seek(-4, os.SEEK_END)
        mlen = struct.unpack("<I", f.read(4))[0]
        f.seek(-4 - mlen, os.SEEK_END)
        man_bytes = f.read(mlen)
        manifest = json.loads(man_bytes)
        bloom_bits = manifest.get("bloom_bits", 2048)
    except Exception:
        bloom_bits = 2048
    bloom_bytes = bloom_bits // 8
    entry_size = ENTRY.size + bloom_bytes
    f.seek(table_pos)
    index = []
    blooms = []
    for _ in range(nb):
        start, count, off, length, xx, sh, bb_len, stats_off = ENTRY.unpack(f.read(ENTRY.size))
        bloom = f.read(bb_len)
        index.append((start, count, off, length))
        blooms.append(bloom)
    return Reader(Path(path), flags, bucket_kb, nb, table_pos, entry_size, index, blooms, f)

def _bucket_for(index: List[Tuple[int, int, int, int]], key: int) -> int:
    """Finds the index of the bucket that potentially contains the given key."""
    lo, hi = 0, len(index) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if index[mid][0] <= key and (mid == len(index) - 1 or index[mid + 1][0] > key):
            return mid
        if index[mid][0] < key:
            lo = mid + 1
        else:
            hi = mid - 1
    return max(0, min(len(index) - 1, lo))

def lookup(reader: Reader, x: int, y: int, z: int, aes_key: Optional[bytes] = None) -> Optional[Tuple[float, float, float]]:
    """Looks up a record by its 3D coordinate (x, y, z) in an MBT2 archive."""
    key = morton64(x, y, z)
    i = _bucket_for(reader.index, key)
    start, count, off, length = reader.index[i]
    bloom = reader.blooms[i]
    if not _bloom_maybe(bloom, key, 8 * len(bloom)):
        return None
    reader.f.seek(off)
    comp = reader.f.read(length)
    comp = _maybe_decrypt(aes_key if (reader.flags & FLAG_AESGCM) else None, comp, struct.pack("<I", i))
    raw = _decompress_data(comp)
    p = 0
    first = struct.unpack_from("<Q", raw, p)[0]
    p += 8
    cur = first
    pos = 0
    while pos < count and cur < key:
        d, p = dec_varint(raw, p)
        cur += d
        pos += 1
    if cur != key:
        return None
    for _ in range(pos, count - 1):
        _, p = dec_varint(raw, p)
    fsz = 4 * count
    masses_off = p
    ents_off = p + fsz
    taus_off = p + 2 * fsz
    j = pos
    mass = struct.unpack_from("<f", raw, masses_off + 4 * j)[0]
    ent = struct.unpack_from("<f", raw, ents_off + 4 * j)[0]
    tau = struct.unpack_from("<f", raw, taus_off + 4 * j)[0]
    return mass, ent, tau

def full_scan(reader: Reader) -> Tuple[int, float]:
    """Performs a full scan of all records in an MBT2 archive."""
    total = 0
    mass_sum = 0.0
    for (start, count, off, length), bloom in zip(reader.index, reader.blooms):
        reader.f.seek(off)
        comp = reader.f.read(length)
        raw = _decompress_data(comp)
        p = 8
        for _ in range(count - 1):
            _, p = dec_varint(raw, p)
        masses = struct.unpack_from(f"<{count}f", raw, p)
        total += count
        mass_sum += float(sum(masses))
    return total, mass_sum

# Example Usage (Optional - uncomment to run)
# if __name__ == "__main__":
#     # Example of creating and reading an MBT2 archive
#     print("Creating example MBT2 archive...")
#     example_records = [
#         (morton64(10, 20, 30), 1.5, 0.1, 0.05),
#         (morton64(5, 15, 25), 2.0, 0.2, 0.1),
#         (morton64(12, 22, 32), 1.8, 0.15, 0.08),
#     ]
#     output_mbt2_file = "example_archive.mbt2"
#     write_mbt2(Path(output_mbt2_file), example_records)
#     print(f"Example archive created: {output_mbt2_file}")

#     print("\nReading from example MBT2 archive...")
#     try:
#         mbt2_reader = open_reader(Path(output_mbt2_file))

#         # Lookup a record
#         x, y, z = 10, 20, 30
#         record = lookup(mbt2_reader, x, y, z)
#         if record:
#             print(f"Lookup for ({x}, {y}, {z}): Mass={record[0]}, Entropy={record[1]}, Tau={record[2]}")
#         else:
#             print(f"Record not found for ({x}, {y}, {z})")

#         # Full scan (example)
#         # total_records, total_mass = full_scan(mbt2_reader)
#         # print(f"Full scan: Total records = {total_records}, Total mass = {total_mass}")

#         mbt2_reader.f.close() # Close the file
#     except Exception as e:
#         print(f"Error reading MBT2 archive: {e}")

#     # Clean up example file
#     if os.path.exists(output_mbt2_file):
#         os.remove(output_mbt2_file)
#         print(f"Removed example archive: {output_mbt2_file}")




"""gmw_tool.py - Stand-alone script providing GMW archive functionality.

This single Python script includes:

1. The MBT2 storage implementation for voxel data (with recommended
   defaults for maximal compaction) - this allows writing and reading
   voxel records using Z-order keys.  The code is derived from
   `mbt_gmw.py` and is included here verbatim for convenience.

2. Convenience functions `compress_folder_to_gmw` and
   `extract_gmw` for packaging arbitrary directory trees into a
   `.gmw` file and extracting them back.  A `.gmw` file in this
   context is a tar archive compressed with Zstandard (if available)
   or zlib and suffixed with the `.gmw` extension.

3. A simple interactive command-line interface that runs when the
   script is executed directly.  The user is prompted to choose
   whether to compress a folder or extract a `.gmw` archive and to
   provide the relevant paths.  This makes the script usable
   out-of-the-box without any additional code.

Usage:
    python gmw_tool.py

The script will prompt the user to select an action and enter
input/output paths.  It relies only on the Python standard library and
optionally the `zstandard` module for faster compression and smaller
archives.  If `zstandard` is not installed, zlib will be used.
"""

import os
import io
import tarfile
import struct
import json
import time
import threading
import zlib
from dataclasses import dataclass
from pathlib import Path
from typing import List, Tuple, Optional, Iterable, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    import zstandard as zstd  # optional
    HAS_ZSTD = True
except Exception:
    zstd = None
    HAS_ZSTD = False

try:
    from cryptography.hazmat.primitives.ciphers.aead import AESGCM  # optional
    HAS_AESGCM = True
except Exception:
    AESGCM = None
    HAS_AESGCM = False

try:
    import xxhash
    HAS_XX = True
except Exception:
    xxhash = None
    HAS_XX = False

import hashlib

# -----------------------------------------------------------------------------
# Morton encoding helpers for voxel keys

_DILATE16 = [0] * 65536

def _init_dilate():
    for i in range(65536):
        n = i
        n = (n | (n << 16)) & 0x0000FFFF0000FFFF
        n = (n | (n << 8)) & 0x00FF00FF00FF00FF
        n = (n | (n << 4)) & 0x0F0F0F0F0F0F0F0F
        n = (n | (n << 2)) & 0x3333333333333333
        n = (n | (n << 1)) & 0x5555555555555555
        _DILATE16[i] = n

_init_dilate()

def morton64(x: int, y: int, z: int) -> int:
    x &= 0x1FFFFF
    y &= 0x1FFFFF
    z &= 0x1FFFFF
    xx = _DILATE16[x & 0xFFFF] | (_DILATE16[(x >> 16) & 0xFFFF] << 32)
    yy = _DILATE16[y & 0xFFFF] | (_DILATE16[(y >> 16) & 0xFFFF] << 32)
    zz = _DILATE16[z & 0xFFFF] | (_DILATE16[(z >> 16) & 0xFFFF] << 32)
    return (xx | (yy << 1) | (zz << 2)) & ((1 << 64) - 1)

def inv_morton64(k: int) -> Tuple[int, int, int]:
    def compact(n: int) -> int:
        n &= 0x5555555555555555
        n = (n | (n >> 1)) & 0x3333333333333333
        n = (n | (n >> 2)) & 0x0F0F0F0F0F0F0F0F
        n = (n | (n >> 4)) & 0x00FF00FF00FF00FF
        n = (n | (n >> 8)) & 0x0000FFFF0000FFFF
        n = (n | (n >> 16)) & 0x00000000FFFFFFFF
        return n
    x = compact(k) & 0x1FFFFF
    y = compact(k >> 1) & 0x1FFFFF
    z = compact(k >> 2) & 0x1FFFFF
    return x, y, z

# Varint helpers

def enc_varint(n: int) -> bytes:
    out = bytearray()
    while True:
        b = n & 0x7F
        n >>= 7
        if n:
            out.append(0x80 | b)
        else:
            out.append(b)
            break
    return bytes(out)

def dec_varint(buf: bytes, off: int) -> Tuple[int, int]:
    shift = 0
    val = 0
    while True:
        b = buf[off]
        off += 1
        val |= (b & 0x7F) << shift
        if not (b & 0x80):
            break
        shift += 7
    return val, off

# Bloom filter hash (deterministic)

def _bloom_hash(key: int, i: int) -> int:
    x = (key + 0x9E3779B97F4A7C15 + i) & 0xFFFFFFFFFFFFFFFF
    x = (x ^ (x >> 30)) * 0xBF58476D1CE4E5B9 & 0xFFFFFFFFFFFFFFFF
    x = (x ^ (x >> 27)) * 0x94D049BB133111EB & 0xFFFFFFFFFFFFFFFF
    x = x ^ (x >> 31)
    return x & 0xFFFFFFFF

def _bloom_add(bits: bytearray, key: int, m: int, h: int = 3) -> None:
    for i in range(h):
        hv = _bloom_hash(key, i) % m
        bits[hv // 8] |= 1 << (hv % 8)

def _bloom_maybe(bits: bytes, key: int, m: int, h: int = 3) -> bool:
    for i in range(h):
        hv = _bloom_hash(key, i) % m
        if not (bits[hv // 8] & (1 << (hv % 8))):
            return False
    return True

# Format constants

MAGIC = b"MBT2\0"
VERSION = 2

FLAG_ZLIB = 1 << 0
FLAG_ZSTD = 1 << 1
FLAG_AESGCM = 1 << 2
FLAG_VARINT = 1 << 3
FLAG_COLUMN = 1 << 4
FLAG_HILBERT = 1 << 5
FLAG_KEY_128 = 1 << 6

HEADER = struct.Struct("<5sB I H I Q")
ENTRY = struct.Struct("<Q I Q I Q 32s H I")
FOOTER = struct.Struct("<I 32s 32s I")

@dataclass
class WriterCfg:
    """Recommended configuration for writing MBT2 archives."""
    target_bucket_kb: int = 128
    use_zstd: bool = True
    zstd_level: int = 3
    threads: int = max(1, os.cpu_count() or 1)
    aesgcm_key: Optional[bytes] = None
    sha256_each: bool = False
    bloom_bits: int = 2048
    column_layout: bool = True

def _compress(raw: bytes, zstd_on: bool, lvl: int = 3) -> bytes:
    if zstd_on and HAS_ZSTD:
        return zstd.ZstdCompressor(level=lvl).compress(raw)
    return zlib.compress(raw, 6)

def _decompress(comp: bytes) -> bytes:
    if HAS_ZSTD and len(comp) >= 4 and comp[:4] == b"\x28\xB5\x2F\xFD":
        return zstd.ZstdDecompressor().decompress(comp)
    return zlib.decompress(comp)

def _maybe_encrypt(aes_key: Optional[bytes], data: bytes, aad: bytes) -> bytes:
    if not aes_key or not HAS_AESGCM:
        return data
    nonce = os.urandom(12)
    return nonce + AESGCM(aes_key).encrypt(nonce, data, aad)

def _maybe_decrypt(aes_key: Optional[bytes], data: bytes, aad: bytes) -> bytes:
    if not aes_key or not HAS_AESGCM:
        return data
    nonce, ct = data[:12], data[12:]
    return AESGCM(aes_key).decrypt(nonce, ct, aad)

def _xx64(b: bytes) -> int:
    if HAS_XX:
        return xxhash.xxh64(b).intdigest()
    return int.from_bytes(hashlib.blake2b(b, digest_size=8).digest(), "little")

def _autotune_bucket_kb(n: int, approx_rec_bytes: int = 20) -> int:
    recs = max(4096, min(12288, n // 80))
    kb = max(64, min(256, (recs * approx_rec_bytes) // 1024))
    return int(kb)

def _pack_bucket(keys: List[int], masses: List[float], ents: List[float], taus: List[float], column: bool = True) -> bytes:
    n = len(keys)
    if n == 0:
        return b""
    out = bytearray()
    out += struct.pack("<Q", keys[0])
    prev = keys[0]
    for i in range(1, n):
        d = keys[i] - prev
        out += enc_varint(d)
        prev = keys[i]
    if column:
        out += struct.pack(f"<{n}f", *masses)
        out += struct.pack(f"<{n}f", *ents)
        out += struct.pack(f"<{n}f", *taus)
    else:
        for i in range(n):
            out += struct.pack("<fff", masses[i], ents[i], taus[i])
    return bytes(out)

def _build_buckets(sorted_recs: List[Tuple[int, float, float, float]], target_kb: int) -> List[Tuple[List[int], List[float], List[float], List[float]]]:
    target = target_kb * 1024
    out = []
    ks: List[int] = []
    ms: List[float] = []
    es: List[float] = []
    ts: List[float] = []
    size = 0
    for k, m, e, t in sorted_recs:
        add = (3 if ks else 8) + 12
        if size + add > target and ks:
            out.append((ks, ms, es, ts))
            ks, ms, es, ts = [], [], [], []
            size = 0
        ks.append(k)
        ms.append(m)
        es.append(e)
        ts.append(t)
        size += add
    if ks:
        out.append((ks, ms, es, ts))
    return out

def write_mbt2(path: Path, records: List[Tuple[int, float, float, float]], cfg: WriterCfg = WriterCfg()) -> Dict:
    path = Path(path)
    t_sort0 = time.perf_counter()
    records.sort(key=lambda r: r[0])
    t_sort = time.perf_counter() - t_sort0
    if cfg.target_bucket_kb <= 0:
        cfg.target_bucket_kb = _autotune_bucket_kb(len(records))
    buckets = _build_buckets(records, cfg.target_bucket_kb)
    nb = len(buckets)
    flags = 0
    flags |= FLAG_ZSTD if (cfg.use_zstd and HAS_ZSTD) else FLAG_ZLIB
    flags |= FLAG_VARINT
    if cfg.column_layout:
        flags |= FLAG_COLUMN
    if cfg.aesgcm_key and HAS_AESGCM:
        flags |= FLAG_AESGCM
    with open(path, "w+b") as f:
        f.write(HEADER.pack(MAGIC, VERSION, flags, cfg.target_bucket_kb, nb, 0))
        header_end = f.tell()
        bloom_bytes = cfg.bloom_bits // 8
        entry_size = ENTRY.size + bloom_bytes
        table_ofs = header_end
        f.seek(table_ofs + nb * entry_size)
        metas = []
        lock = threading.Lock()
        def write_bucket(i: int, b) -> None:
            ks, ms, es, ts = b
            raw = _pack_bucket(ks, ms, es, ts, column=bool(flags & FLAG_COLUMN))
            comp = _compress(raw, zstd_on=bool(flags & FLAG_ZSTD), lvl=cfg.zstd_level)
            comp = _maybe_encrypt(cfg.aesgcm_key if (flags & FLAG_AESGCM) else None, comp, aad=struct.pack("<I", i))
            xx = _xx64(comp)
            sh = hashlib.sha256(comp).digest() if cfg.sha256_each else b"\x00" * 32
            bloom = bytearray(bloom_bytes)
            for kk in ks:
                _bloom_add(bloom, kk, cfg.bloom_bits)
            with lock:
                pos = f.tell()
                f.write(comp)
                metas.append((ks[0], len(ks), pos, len(comp), xx, sh, bytes(bloom)))
        t_comp0 = time.perf_counter()
        with ThreadPoolExecutor(max_workers=cfg.threads) as ex:
            futs = [ex.submit(write_bucket, i, b) for i, b in enumerate(buckets)]
            for fu in as_completed(futs):
                fu.result()
        t_comp = time.perf_counter() - t_comp0
        metas.sort(key=lambda m: m[0])
        data_end = f.tell()
        f.seek(table_ofs)
        for (start, count, off, length, xx, sh, bloom) in metas:
            f.write(ENTRY.pack(start, count, off, length, xx, sh, len(bloom), 0))
            f.write(bloom)
        f.seek(table_ofs)
        table_blob = f.read(nb * entry_size)
        table_hash = hashlib.sha256(table_blob).digest()
        leaves = [sh if cfg.sha256_each else xx.to_bytes(8, "little") for (_, _, _, _, xx, sh, _) in metas]
        if leaves:
            cur = [hashlib.sha256(x).digest() if len(x) != 32 else x for x in leaves]
            while len(cur) > 1:
                nxt = []
                for i in range(0, len(cur), 2):
                    a = cur[i]
                    b = cur[i + 1] if i + 1 < len(cur) else a
                    nxt.append(hashlib.sha256(a + b).digest())
                cur = nxt
            merkle = cur[0]
        else:
            merkle = b"\x00" * 32
        f.seek(data_end)
        feature_crc = zlib.crc32(struct.pack("<IHH", VERSION, cfg.target_bucket_kb, flags)) & 0xFFFFFFFF
        f.write(FOOTER.pack(nb * entry_size, table_hash, merkle, feature_crc))
        end_pos = f.tell()
        f.seek(0)
        f.write(HEADER.pack(MAGIC, VERSION, flags, cfg.target_bucket_kb, nb, table_ofs))
        f.seek(end_pos)
        manifest = {
            "nbuckets": nb,
            "schema": 0,
            "flags": flags,
            "target_bucket_kb": cfg.target_bucket_kb,
            "compressor": "zstd" if (flags & FLAG_ZSTD) else "zlib",
            "aesgcm": bool(flags & FLAG_AESGCM),
            "column": bool(flags & FLAG_COLUMN),
            "varint": bool(flags & FLAG_VARINT),
            "bloom_bits": cfg.bloom_bits,
        }
        man_bytes = json.dumps(manifest, separators=(",", ":")).encode("utf-8")
        f.write(man_bytes)
        f.write(struct.pack("<I", len(man_bytes)))
    return {
        "file_mb": os.path.getsize(path) / (1024 * 1024),
        "nbuckets": nb,
        "bucket_kb": cfg.target_bucket_kb,
        "flags": flags,
    }

@dataclass
class Reader:
    path: Path
    flags: int
    bucket_kb: int
    table_pos: int
    nbuckets: int
    entry_size: int
    index: List[Tuple[int, int, int, int]]
    blooms: List[bytes]
    f: io.BufferedReader

def open_reader(path: Path) -> Reader:
    f = open(path, "rb")
    hdr = f.read(HEADER.size)
    magic, ver, flags, bucket_kb, nb, table_pos = HEADER.unpack(hdr)
    assert magic == MAGIC and ver == VERSION, "Invalid MBT2 file"
    try:
        f.seek(-4, os.SEEK_END)
        mlen = struct.unpack("<I", f.read(4))[0]
        f.seek(-4 - mlen, os.SEEK_END)
        man_bytes = f.read(mlen)
        manifest = json.loads(man_bytes)
        bloom_bits = manifest.get("bloom_bits", 2048)
    except Exception:
        bloom_bits = 2048
    bloom_bytes = bloom_bits // 8
    entry_size = ENTRY.size + bloom_bytes
    f.seek(table_pos)
    index = []
    blooms = []
    for _ in range(nb):
        start, count, off, length, xx, sh, bb, stats_off = ENTRY.unpack(f.read(ENTRY.size))
        bloom = f.read(bb)
        index.append((start, count, off, length))
        blooms.append(bloom)
    return Reader(Path(path), flags, bucket_kb, table_pos, nb, entry_size, index, blooms, f)

def _bucket_for(index: List[Tuple[int, int, int, int]], key: int) -> int:
    lo, hi = 0, len(index) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if index[mid][0] <= key and (mid == len(index) - 1 or index[mid + 1][0] > key):
            return mid
        if index[mid][0] < key:
            lo = mid + 1
        else:
            hi = mid - 1
    return max(0, min(len(index) - 1, lo))

def lookup(reader: Reader, x: int, y: int, z: int, aes_key: Optional[bytes] = None) -> Optional[Tuple[float, float, float]]:
    key = morton64(x, y, z)
    i = _bucket_for(reader.index, key)
    start, count, off, length = reader.index[i]
    bloom = reader.blooms[i]
    if not _bloom_maybe(bloom, key, 8 * len(bloom)):
        return None
    reader.f.seek(off)
    comp = reader.f.read(length)
    comp = _maybe_decrypt(aes_key if (reader.flags & FLAG_AESGCM) else None, comp, struct.pack("<I", i))
    raw = _decompress(comp)
    p = 0
    first = struct.unpack_from("<Q", raw, p)[0]
    p += 8
    cur = first
    pos = 0
    while pos < count and cur < key:
        d, p = dec_varint(raw, p)
        cur += d
        pos += 1
    if cur != key:
        return None
    for _ in range(pos, count - 1):
        _, p = dec_varint(raw, p)
    fsz = 4 * count
    masses_off = p
    ents_off = p + fsz
    taus_off = p + 2 * fsz
    j = pos
    mass = struct.unpack_from("<f", raw, masses_off + 4 * j)[0]
    ent = struct.unpack_from("<f", raw, ents_off + 4 * j)[0]
    tau = struct.unpack_from("<f", raw, taus_off + 4 * j)[0]
    return mass, ent, tau

def full_scan(reader: Reader) -> Tuple[int, float]:
    total = 0
    mass_sum = 0.0
    for (start, count, off, length), bloom in zip(reader.index, reader.blooms):
        reader.f.seek(off)
        comp = reader.f.read(length)
        raw = _decompress(comp)
        p = 8
        for _ in range(count - 1):
            _, p = dec_varint(raw, p)
        masses = struct.unpack_from(f"<{count}f", raw, p)
        total += count
        mass_sum += float(sum(masses))
    return total, mass_sum

# -----------------------------------------------------------------------------
# Generic .gmw archive helpers

def compress_folder_to_gmw(folder_path: str, output_path: str, use_zstd: bool = True, zstd_level: int = 3) -> None:
    folder = Path(folder_path)
    output = Path(output_path)
    assert folder.is_dir(), f"{folder_path} is not a directory"
    buf = io.BytesIO()
    with tarfile.open(fileobj=buf, mode="w") as tf:
        tf.add(folder, arcname=".")
    tar_data = buf.getvalue()
    if use_zstd and HAS_ZSTD:
        comp = zstd.ZstdCompressor(level=zstd_level).compress(tar_data)
    else:
        comp = zlib.compress(tar_data, 6)
    with open(output, "wb") as out_f:
        out_f.write(comp)

def extract_gmw(gmw_path: str, output_dir: str) -> None:
    gmw_file = Path(gmw_path)
    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    comp_data = gmw_file.read_bytes()
    if HAS_ZSTD and len(comp_data) >= 4 and comp_data[:4] == b"\x28\xB5\x2F\xFD":
        tar_data = zstd.ZstdDecompressor().decompress(comp_data)
    else:
        tar_data = zlib.decompress(comp_data)
    with io.BytesIO(tar_data) as buf:
        with tarfile.open(fileobj=buf, mode="r") as tf:
            tf.extractall(path=out_dir)

# -----------------------------------------------------------------------------
# Simple interactive command-line interface

def main() -> None:
    print("GMW Archive Tool")
    print("=================")
    print("1) Compress a folder into a .gmw archive")
    print("2) Extract a .gmw archive into a folder")
    choice = input("Choose an action (1/2): ").strip()
    if choice == "1":
        inp = input("Enter the path of the folder to compress: ").strip().strip('"')
        outp = input("Enter the output .gmw file name: ").strip().strip('"')
        if not outp.endswith(".gmw"):
            outp += ".gmw"
        try:
            compress_folder_to_gmw(inp, outp, use_zstd=True)
            print(f"Folder '{inp}' compressed into '{outp}'.")
        except Exception as e:
            print("Error during compression:", e)
    elif choice == "2":
        inp = input("Enter the path of the .gmw file to extract: ").strip().strip('"')
        outp = input("Enter the output folder: ").strip().strip('"')
        try:
            extract_gmw(inp, outp)
            print(f"Archive '{inp}' extracted into '{outp}'.")
        except Exception as e:
            print("Error during extraction:", e)
    else:
        print("Invalid choice. Exiting.")


main()




import os
import tarfile
import zlib
import hashlib
import struct
import json
from pathlib import Path
from typing import Optional, Dict
from tqdm import tqdm # Import tqdm for progress bar
import io # Import io for in-memory tar operations

# Optional imports for Zstandard and xxHash
try:
    import zstandard as zstd
    HAS_ZSTD = True
except ImportError:
    zstd = None
    HAS_ZSTD = False

try:
    import xxhash
    HAS_XX = True
except ImportError:
    xxhash = None
    HAS_XX = False

# Note: MBT2 specific functions (morton encoding, varint, bloom filter, WriterCfg, MBT2Record, _pack_bucket, _build_buckets, write_mbt2)
# and AESGCM related functions are not included here as they are specific to the MBT2 format
# and not part of the generic .gmw archiving tool as requested.

def compress_folder_to_gmw(folder_path: str, output_path: str, use_zstd: bool = True, zstd_level: int = 3) -> None:
    """
    Compresses a folder into a .gmw archive using tar and optionally Zstandard.

    Args:
        folder_path: Path to the folder to compress.
        output_path: Path for the output .gmw file.
        use_zstd: Whether to use Zstandard compression (defaults to True).
                  If False or zstandard library is not available, zlib will be used.
        zstd_level: Zstandard compression level (1-19, defaults to 3).
                    Only used if use_zstd is True and zstandard is available.
    """
    folder = Path(folder_path)
    output = Path(output_path)
    assert folder.is_dir(), f"Error: {folder_path} is not a directory"

    buf = io.BytesIO()
    # Estimate the number of files for tqdm progress bar
    file_count = 0
    for _, _, files in os.walk(folder):
        file_count += len(files)

    with tarfile.open(fileobj=buf, mode="w") as tf:
        with tqdm(total=file_count, unit='file', desc=f'Compressing {folder.name}') as pbar:
            for root, _, files in os.walk(folder):
                for file in files:
                    file_path = Path(root) / file
                    # Add file to tar archive, preserving relative path
                    tf.add(file_path, arcname=file_path.relative_to(folder))
                    pbar.update(1)

    tar_data = buf.getvalue()

    compressor_name = "zlib"
    comp_data = zlib.compress(tar_data, 6) # Default to zlib level 6

    if use_zstd and HAS_ZSTD:
        try:
            comp_data = zstd.ZstdCompressor(level=zstd_level).compress(tar_data)
            compressor_name = "zstd"
            print(f"Using Zstandard compression (level {zstd_level}).")
        except Exception as e:
            print(f"Warning: Zstandard compression failed ({e}). Falling back to zlib.")
            # Use zlib compression if zstandard fails
            comp_data = zlib.compress(tar_data, 6)
            compressor_name = "zlib"
            print("Using zlib compression.")
    else:
        print("Using zlib compression.")


    # Calculate checksum of compressed data (using sha256 as a robust default)
    checksum = hashlib.sha256(comp_data).hexdigest()
    checksum_algorithm = "sha256"

    # Create metadata dictionary
    metadata: Dict[str, Optional[str or int]] = {
        "compressor": compressor_name,
        "zstd_level": zstd_level if compressor_name == "zstd" else None,
        "checksum": checksum,
        "checksum_algorithm": checksum_algorithm
    }

    # Serialize metadata to JSON and encode
    metadata_bytes = json.dumps(metadata, separators=(",", ":")).encode("utf-8")
    metadata_length = struct.pack("<I", len(metadata_bytes))

    # Write compressed data, metadata, and metadata length to the output file
    with open(output, "wb") as out_f:
        out_f.write(comp_data)
        out_f.write(metadata_bytes)
        out_f.write(metadata_length)

    print(f"Successfully compressed '{folder_path}' to '{output_path}' using {compressor_name}.")


def extract_gmw(gmw_path: str, output_dir: str) -> None:
    """
    Extracts a .gmw archive to a specified directory.

    Args:
        gmw_path: Path to the .gmw archive file.
        output_dir: Directory to extract the contents to.
    """
    gmw_file = Path(gmw_path)
    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    if not gmw_file.is_file():
        print(f"Error: Archive file not found at '{gmw_path}'")
        return

    try:
        with open(gmw_file, "rb") as in_f:
            # Read metadata length from the end of the file
            in_f.seek(-4, os.SEEK_END)
            metadata_length = struct.unpack("<I", in_f.read(4))[0]

            # Read metadata
            in_f.seek(-4 - metadata_length, os.SEEK_END)
            metadata_bytes = in_f.read(metadata_length)
            metadata = json.loads(metadata_bytes)

            # Get compressor information and expected checksum
            compressor_name = metadata.get("compressor")
            expected_checksum = metadata.get("checksum")
            checksum_algorithm = metadata.get("checksum_algorithm", "sha256") # Default to sha256

            # Read compressed data (excluding metadata and its length)
            in_f.seek(0)
            comp_data = in_f.read(os.path.getsize(gmw_file) - metadata_length - 4)

        # Verify checksum of the compressed data
        actual_checksum = None
        if checksum_algorithm == "sha256":
            actual_checksum = hashlib.sha256(comp_data).hexdigest()
        elif checksum_algorithm == "xxhash64" and HAS_XX:
             actual_checksum = xxhash.xxh64(comp_data).hexdigest()
        else:
            print(f"Warning: Unsupported or unavailable checksum algorithm '{checksum_algorithm}'. Skipping checksum verification.")

        if actual_checksum and actual_checksum != expected_checksum:
            raise ValueError(f"Checksum mismatch! Archive may be corrupted. Expected: {expected_checksum}, Got: {actual_checksum}")
        elif actual_checksum:
            print("Checksum verification successful.")


        # Decompress data based on metadata
        tar_data = None
        if compressor_name == "zstd" and HAS_ZSTD:
            try:
                tar_data = zstd.ZstdDecompressor().decompress(comp_data)
                print("Using Zstandard decompression.")
            except Exception as e:
                 print(f"Error during Zstandard decompression: {e}")
        elif compressor_name == "zlib":
            try:
                tar_data = zlib.decompress(comp_data)
                print("Using zlib decompression.")
            except Exception as e:
                 print(f"Error during zlib decompression: {e}")
        else:
            print(f"Error: Unsupported or unavailable compressor '{compressor_name}' specified in metadata.")
            return # Exit if decompression method is not supported

        if tar_data is None:
             print("Error: Failed to decompress data.")
             return

        # Extract tar data with progress bar
        with io.BytesIO(tar_data) as buf:
            with tarfile.open(fileobj=buf, mode="r") as tf:
                total_members = len(tf.getmembers())
                # Check if total_members is reasonable before creating tqdm
                if total_members > 0:
                    with tqdm(total=total_members, unit='file', desc=f'Extracting to {out_dir.name}') as pbar:
                        # Mitigate CVE-2007-4559 - ensure extraction is within the output directory
                        for member in tf.getmembers():
                            # Ensure member path is relative and within the intended output directory
                            member_path = Path(out_dir) / member.name
                            if not member_path.resolve().is_relative_to(out_dir.resolve()):
                                 print(f"Skipping potentially malicious path during extraction: {member.name}")
                                 continue
                            tf.extract(member, path=out_dir)
                            pbar.update(1)
                else:
                     # If no members, just extract all (handles empty archives or unexpected structures)
                     tf.extractall(path=out_dir)
                     print(f"Extracted archive '{gmw_path}' to '{out_dir}' (no distinct files/members found in tar header).")


        print(f"Successfully extracted '{gmw_path}' to '{output_dir}'.")

    except Exception as e:
        print(f"An error occurred during extraction: {e}")

                          
                          

                          
